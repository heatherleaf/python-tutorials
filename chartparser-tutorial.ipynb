{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Earley-style chart parsing made easy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(by Peter Ljunglöf, peter.ljunglof@gu.se)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chart parsing is a pretty old technique, originating in the 60's with CKY (Cocke-Kasami-Younger). CKY is a really simple and efficient algorithm, but it only works for grammars in Chomsky normal form. The first efficient parsing algorithm for general context-free grammars was [Jay Earley's algorithm from 1970](https://doi.org/10.1145/362007.362035), and that algorithm is what I will try to explain in this notebook.\n",
    "\n",
    "Earley's algorithm is still the best there is in terms of complexity – no practical context-free parsing algorithm has beaten its cubic complexity – but there has been several variants, extensions and optimisations. But Earley's algorithm serves as a very good starting point.\n",
    "\n",
    "One thing that has bothered me for a long time is that all tutorials on chart parsing and in particular Earley's algorithm are so complicated, or they don't explain the details on what you have to do to make it efficient.\n",
    "\n",
    "(A boastful side note) One exception is my own chart parsing tutorial from 2002, but it was for the programming language Haskell. The tutorial itself is chapter 5 in [my licenciate thesis](https://gup.ub.gu.se/publication/10783), and the code is in [a GitHub repo](https://github.com/heatherleaf/haskell-functional-parsing/tree/master/algorithms).\n",
    "\n",
    "So, here is a notebook that will try to explain the Earley chart parsing algorithm, and how to implement it in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use named tuples for a quick-and-dirty way of creating new classes. The `islice` function will be used for taking a prefix of a generator later. None of them are really important for the parsing algorithms themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonterminals and terminals\n",
    "\n",
    "We assume knowledge of context-free grammars. Context-free categories (nonterminals) are represented as strings, just as words (terminals).\n",
    "\n",
    "This means that we don't make a distinction between nonterminals (the categories) and terminals (the words or tokens). We happen to use capitals for the nonterminals and lowercase terminals, but that's only by convention. To enforce a strict division of the symbols into nonterminals and terminals only complicate the implementations.\n",
    "\n",
    "It is quite straightforward to impose this distinction if you want to, so this is left as an exercise for the interested reader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammar rules\n",
    "\n",
    "A context-free rule *A –> B1 ... Bn* is represented as a pair, where the second element is a sequence of the right-hand side categories. To make it more readable, we define it as a named tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rule = namedtuple('Rule', \"lhs rhs\")\n",
    "Rule.__str__ = lambda self: \"%s --> %s\" % (self.lhs, \" \".join(map(str, self.rhs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we redefine the pretty-printing of rules, so that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "str(r) = S --> NP VP\n",
      "repr(r) = Rule(lhs='S', rhs=('NP', 'VP'))\n",
      "r.lhs = S, r.rhs = ('NP', 'VP')\n"
     ]
    }
   ],
   "source": [
    "r = Rule('S', ('NP', 'VP'))\n",
    "print('str(r) = %s' % (r,))\n",
    "print('repr(r) = %r' % (r,))\n",
    "print('r.lhs = %s, r.rhs = %s' % (r.lhs, r.rhs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context-free grammar\n",
    "\n",
    "A context-free grammar is simply a collection of grammar rules. Here is the example grammar that wewill use throughout this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = [ Rule('S',   ('NP','VP')),\n",
    "            Rule('VP',  ('Verb',)),       Rule('VP',  ('Verb','NP')),   Rule('VP', ('VP','PP')),\n",
    "            Rule('NP',  ('Det','Noun')),  Rule('NP', ('NP','PP')),\n",
    "            Rule('PP',  ('Prep','NP')),\n",
    "            Rule('Verb',('sees',)),\n",
    "            Rule('Det', ('the',)),    Rule('Det', ('a',)),\n",
    "            Rule('Prep',('under',)),  Rule('Prep',('with',)),   Rule('Prep',('in',)), \n",
    "            Rule('Noun',('zebra',)),  Rule('Noun',('lion',)),   Rule('Noun',('tree',)),\n",
    "            Rule('Noun',('park',)),   Rule('Noun',('telescope',)),\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S --> NP VP\n",
      "VP --> Verb\n",
      "VP --> Verb NP\n",
      "VP --> VP PP\n",
      "NP --> Det Noun\n",
      "NP --> NP PP\n",
      "PP --> Prep NP\n",
      "Verb --> sees\n",
      "Det --> the\n",
      "Det --> a\n",
      "Prep --> under\n",
      "Prep --> with\n",
      "Prep --> in\n",
      "Noun --> zebra\n",
      "Noun --> lion\n",
      "Noun --> tree\n",
      "Noun --> park\n",
      "Noun --> telescope\n"
     ]
    }
   ],
   "source": [
    "for rule in grammar:\n",
    "    print(rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left corners\n",
    "\n",
    "Later we will need a way to find the grammar rules we want efficiently. The Earley algorithm looks for grammar rules whose right-hand sides start with a given symbol. This symbol is called the *left corner* of the rule.\n",
    "\n",
    "As an example, given the category NP, the matching rules from the example grammar are *S –> NP VP* and *NP –> NP PP*.\n",
    "\n",
    "The following function transforms a grammar into a Python dictionary that returns all matching rules for a given left corner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leftcorner_dict(grammar):\n",
    "    leftcorners = {}\n",
    "    for rule in grammar:\n",
    "        leftcorners.setdefault(rule.rhs[0], []).append(rule)\n",
    "    return leftcorners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP         :  S --> NP VP       NP --> NP PP      \n",
      "Verb       :  VP --> Verb       VP --> Verb NP    \n",
      "VP         :  VP --> VP PP      \n",
      "Det        :  NP --> Det Noun   \n",
      "Prep       :  PP --> Prep NP    \n",
      "sees       :  Verb --> sees     \n",
      "the        :  Det --> the       \n",
      "a          :  Det --> a         \n",
      "under      :  Prep --> under    \n",
      "with       :  Prep --> with     \n",
      "in         :  Prep --> in       \n",
      "zebra      :  Noun --> zebra    \n",
      "lion       :  Noun --> lion     \n",
      "tree       :  Noun --> tree     \n",
      "park       :  Noun --> park     \n",
      "telescope  :  Noun --> telescope\n"
     ]
    }
   ],
   "source": [
    "for lc, rules in leftcorner_dict(grammar).items():\n",
    "    print(\"%-10s\" % (lc,), \": \", \"\".join(\"%-18s\" % (r,) for r in rules))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topdown rules\n",
    "\n",
    "Another way of finding grammar rules is to search for a given left-hand side. So here's the corresponding function that transforms a grammar into a dictionary that returns all matching rules for a given left-hand side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topdown_dict(rules):\n",
    "    topdowns = {}\n",
    "    for r in rules:\n",
    "        topdowns.setdefault(r.lhs, []).append(r)\n",
    "    return topdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S          :  S --> NP VP       \n",
      "VP         :  VP --> Verb       VP --> Verb NP    VP --> VP PP      \n",
      "NP         :  NP --> Det Noun   NP --> NP PP      \n",
      "PP         :  PP --> Prep NP    \n",
      "Verb       :  Verb --> sees     \n",
      "Det        :  Det --> the       Det --> a         \n",
      "Prep       :  Prep --> under    Prep --> with     Prep --> in       \n",
      "Noun       :  Noun --> zebra    Noun --> lion     Noun --> tree     Noun --> park     Noun --> telescope\n"
     ]
    }
   ],
   "source": [
    "for lhs, rules in topdown_dict(grammar).items():\n",
    "    print(\"%-10s\" % (lhs,), \": \", \"\".join(\"%-18s\" % (r,) for r in rules))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No empty rules\n",
    "\n",
    "Note that empty rules (i.e., rules with empty right-hand sides) are forbidden! The reason for this is to make the code as compact and readable as possible. This is a disadvantage, because sometimes it's really easier to write grammars using empty rules. \n",
    "\n",
    "Fortunately there are methods for transforming grammars with empty rules, into grammars without. Alternatively, the algorithms presented here can be augmented to cope with empty rules. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example sentences\n",
    "\n",
    "We also need some example sentences to play around with. Here is an ever-growing list of sentences showing off the [PP-attachment problem](https://languagelog.ldc.upenn.edu/nll/?p=17711):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example(n):\n",
    "    prefix = \"the lion sees a zebra\".split()\n",
    "    suffix = \"under a tree with a telescope in the park\".split()\n",
    "    return prefix + (suffix * (n//3+1))[:n*3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example(0) = the lion sees a zebra\n",
      "example(1) = the lion sees a zebra under a tree\n",
      "example(2) = the lion sees a zebra under a tree with a telescope\n",
      "example(3) = the lion sees a zebra under a tree with a telescope in the park\n",
      "example(4) = the lion sees a zebra under a tree with a telescope in the park under a tree\n"
     ]
    }
   ],
   "source": [
    "for n in range(5):\n",
    "    print(\"example(%d) = %s\" % (n, \" \".join(example(n))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse trees\n",
    "\n",
    "We take the simplistic approach to parse trees: A tree is a pair of a root node and a sequence of its children. To make them more readable, we define them as a named tuple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tree = namedtuple('Tree', \"root  children\")\n",
    "Tree.__str__ = lambda tree: ( \"%s[%s]\" % (tree.root, \" \".join(map(str, tree.children))) \n",
    "                              if tree.children else str(tree.root) \n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repr(tree): Tree(root='S', children=(Tree(root='NP', children=(Tree(root='Det', children=(Tree(root='the', children=()),)), Tree(root='Noun', children=(Tree(root='cat', children=()),)))), Tree(root='VP', children=(Tree(root='Verb', children=(Tree(root='sleeps', children=()),)),))))\n",
      "\n",
      "str(tree): S[NP[Det[the] Noun[cat]] VP[Verb[sleeps]]]\n"
     ]
    }
   ],
   "source": [
    "tree = Tree('S', (Tree('NP', (Tree('Det',  (Tree('the', ()),)),\n",
    "                              Tree('Noun', (Tree('cat', ()),)),)), \n",
    "                  Tree('VP', (Tree('Verb', (Tree('sleeps', ()),)),)),))\n",
    "print(\"repr(tree):\", repr(tree))\n",
    "print(\"\\nstr(tree):\", str(tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edges (or items, or states)\n",
    "\n",
    "The basic element in Earley parsing, and in chart parsing in general, is the *edge*, also known as an item or a state. The edge consists of a span and a dotted rule, which tells that a part of a grammar rule has been found spanning some part of the input. The following is the general form for an edge:\n",
    "\n",
    "> [*i–k*: *A –> B1 ... Bm • Bn ... Bp*]\n",
    "\n",
    "The meaning of an edge is twofold:\n",
    "\n",
    "1. What has been found: \"the categories *B1...Bm* have been found spanning the words *i*+1, *i*+2, ..., *k* in the input sentence\". \n",
    "2. What we look for: \"if we find the categories *Bn...Bp* spanning the words *k*+1, ..., *r*, then we know that there is a *A* spanningn the words *i*+1, *r*\".\n",
    "\n",
    "The idea with chart parsing is that we advance the right-hand side of a grammar rule one symbol at the time, and the dot denotes how far we have come. Another idea is to not do unnecessary work, so if there are two possible ways of reaching a parsing state, we do not have to the same work twice.\n",
    "\n",
    "In Python we can define edges as a 6-tuple, consisting of the span (`start` and `end`), the grammar rule (`lhs` and `rhs`), the dot position (`dot`), and finally an optional parse result (`result`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [],
   "source": [
    "Edge = namedtuple('Edge', \"start end lhs rhs dot result\", defaults=[None])\n",
    "Edge.__str__ = lambda e: ( \"[%s-%s: %s --> %s . %s]\" \n",
    "                           % (e.start, e.end, e.lhs, \" \".join(e.rhs[:e.dot]), \" \".join(e.rhs[e.dot:]))\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we do not show the optional result when pretty-printing the edge. This is because it can grow quite large, and then the edge becomes unreadable.\n",
    "\n",
    "The following shows two edges for the same grammar rule. If the example sentence is \"*the lion sees a zebra under a tree*\", then the first one says that it has found an NP spanning the words \"*the lion*\", while the second one says that it has found an NP followed by a VP spannning the words \"*the lion sees a zebra*\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0-2: S --> NP . VP]\n",
      "[0-5: S --> NP VP . ]\n"
     ]
    }
   ],
   "source": [
    "edge1 = Edge(0, 2, 'S', ('NP', 'VP'), 1); print(edge1)\n",
    "edge2 = Edge(0, 5, 'S', ('NP', 'VP'), 2); print(edge2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second edge has a special status, because it is complete, or *passive*. This edge says that it has found a complete S phrase between positions 0–5, and that this phrase is derived from the grammar rule S –> NP VP.\n",
    "\n",
    "The following method is a test whether an edge is passive or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "Edge.passive = lambda e: e.dot == len(e.rhs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The chart\n",
    "\n",
    "Concneptually, the *chart* is a collection of edges that we build while parsing. When parsing is complete, the chart consists of all edges that can be inferred from the input sentence. \n",
    "\n",
    "After that we can extract the parse trees (or whatever information we want) from the final chart. For extracting parse trees it's enough to only consider the passive edges in the chart, so our parsing functions will only return the passive edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Earley parsing\n",
    "\n",
    "The main idea with Earley parsing is that we go through the input one word at the time, and calculate all possible edges that can be inferred after seeing this new word. This means that we can partition the chart into edge-sets, one per input word (or rather input position). So, the chart is therefore an array of edgesets – we call this an Earley chart.\n",
    "\n",
    "The Earley parser consists of three \"inference steps\", called **Scan**, **Predict** and **Complete**, which all adds edges to an edgeset:\n",
    "\n",
    "- **Scan**: Add a passive edge [*j–k*: *w –> •*] to edgeset *k* for every word *w* between positions *j* and *k* (i.e., where *j* is *k*-1).\n",
    "\n",
    "- **Predict**: If there is a passive edge [*j–k*: *B –> ... •*] in edgeset *k*, and there is a grammar rule *A –> B C ...* looking for a *B*, then add a new edge [*j–k*: *A –> B • C ...*] to edgeset *k*.\n",
    "\n",
    "- **Complete**: If there is a passive edge [*j–k*: *B –> ... •*] in edgeset *k*, and there is an active edge [*i–j*: *A –> ... • B C ...*] in edgeset *k*, then add a new edge [*i–k*: *A –> B • C ...*] to edgeset *k*.\n",
    "\n",
    "Note that **Predict** and **Complete** have a *trigger edge* from the same edgeset *k*. This means that the set already has to be nonempty for them to be able to add new edges. The edgeset is initialised by **Scan**, and then **Predict** and **Complete** can act until non new edges can be added. \n",
    "\n",
    "Another way to view this is that edgeset *k* is the set of all inferences that we can draw about phrases ending in word *k*, from the grammar and the input sentence. **Scan** is then an axiom, and **Predict** and **Complete** are inference rules. We initialise our knowledge from the axiom, and then apply the inference rules until we reach a fixpoint.\n",
    "\n",
    "One way to implement this is to use [fixed-point recursion](https://en.wikipedia.org/wiki/Fixed-point_combinator), which is very easy to do in a functional programming langauge (see e.g. [my Haskell implementations](https://github.com/heatherleaf/haskell-functional-parsing/tree/master/algorithms)). Here we will instead use a todo-list (an \"agenda\"). The agenda contains the items that should be added to the edgeset but have not been processed by the inference rules yet. When the agenda becomes empty, the edgeset is completed. The agenda is iniatialised by the **Scan** axiom, and then we use a while-loop where we apply the **Predict** and **Complete** inference rules.\n",
    "\n",
    "This technique is called [dynamic programming](https://en.m.wikipedia.org/wiki/Dynamic_programming)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation 1\n",
    "\n",
    "For each input word and position *k*, we initialise the agenda to a single passive edge. Then we repeat removing one edge from the agenda until it is exhausted. We add it to the edgeset (if it's not already there), and if it is passive, we apply the predictor and the scanner, adding the new edges to the agenda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {},
   "outputs": [],
   "source": [
    "def earley1(grammar, input):\n",
    "    # The 0th edgeset is always empty, because there are no edges ending in position 0\n",
    "    chart = [set()]\n",
    "    \n",
    "    # Enumerate each word in the input, starting from k=1\n",
    "    for k, word in enumerate(input, 1):\n",
    "        edgeset = set()\n",
    "        \n",
    "        # Scan: add one single passive edge for the input word\n",
    "        agenda = [ Edge(k-1, k, word, (), 0) ]\n",
    "        \n",
    "        while agenda:\n",
    "            edge = agenda.pop()\n",
    "            \n",
    "            # Add the edge to the edgeset (if it's not already there)\n",
    "            if edge not in edgeset:\n",
    "                edgeset.add(edge)\n",
    "                \n",
    "                # If the edge is passive we can apply the inference rules\n",
    "                if edge.passive():\n",
    "                    \n",
    "                    # Predict: find grammar rules looking for edge.lhs\n",
    "                    agenda.extend(\n",
    "                        Edge(edge.start, k, lhs, rhs, 1)\n",
    "                        for (lhs, rhs) in grammar \n",
    "                        if edge.lhs == rhs[0]\n",
    "                    )\n",
    "                    \n",
    "                    # Complete: find active edges in old edgesets looking for edge.lhs\n",
    "                    agenda.extend(\n",
    "                        Edge(e.start, k, e.lhs, e.rhs, e.dot+1) \n",
    "                        for e in chart[edge.start] \n",
    "                        if not e.passive() \n",
    "                        if edge.lhs == e.rhs[e.dot]\n",
    "                    )\n",
    "        \n",
    "        # Add the edgeset to the end of the chart\n",
    "        chart.append(edgeset)\n",
    "    \n",
    "    # Filter all passive edges from the chart, and return them\n",
    "    return [ [e for e in edgeset if e.passive()] \n",
    "             for edgeset in chart \n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what can we do with the parse result – the chart? Well, for one we can test if the parsing succeeded, by looking for a passive edge spanning the whole input, with left-hand side being the starting category of the grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [],
   "source": [
    "def success(chart, cat, start=0, end=-1):\n",
    "    return any(\n",
    "        edge.start == start and edge.lhs == cat and edge.passive()\n",
    "        for edge in chart[end]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing \"the lion sees a zebra under a tree with a telescope in the park\": True\n",
      "Parsing \"the lion sees a zebra under\": False\n"
     ]
    }
   ],
   "source": [
    "sent1 = example(3); sent2 = sent1[:6]\n",
    "print('Parsing \"%s\": %s' % (' '.join(sent1), success(earley1(grammar, sent1), 'S')))\n",
    "print('Parsing \"%s\": %s' % (' '.join(sent2), success(earley1(grammar, sent2), 'S')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing we can do is to return the number of edges in the chart (which will be useful when we try to debug the implementations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chartsize(chart):\n",
    "    return sum(map(len, chart))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the edges in the chart. Here is a generic version where we can specify what edgesets should be printed, and a limit on the number of edges printed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_chart(chart, positions=None, cutoff=None):\n",
    "    print(\"Chart size: %d edges\" % chartsize(chart))\n",
    "    for (k, edgeset) in enumerate(chart):\n",
    "        if edgeset and (positions is None or k in positions or (k-len(chart)) in positions):\n",
    "            print(\"%d edges ending in position %s:\" % (len(edgeset), k))\n",
    "            for n, edge in enumerate(sorted(edgeset)):\n",
    "                if cutoff and n >= cutoff:\n",
    "                    print(\"    ...\")\n",
    "                    break\n",
    "                print(\"   \", edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to test our parsers on some input. The first argument is the parsing function (i.e, `earley1` above), and the rest are hopefully self-explanatory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(parser, grammar, cat, sentence, positions=None, cutoff=8):\n",
    "    nwords = len(sentence)\n",
    "    if nwords <= 15:\n",
    "        print('Parsing %d words: \"%s\"' % (nwords, \" \".join(sentence)))\n",
    "    else:\n",
    "        print('Parsing %d words: \"%s ... %s\"' \n",
    "              % (nwords, \" \".join(sentence[:3]), \" \".join(sentence[-9:])))\n",
    "    chart = parser(grammar, sentence)\n",
    "    print(\"Yay, success!!\" if success(chart, cat) else \"Meh, failure:(\")\n",
    "    print_chart(chart, positions, cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 14 words: \"the lion sees a zebra under a tree with a telescope in the park\"\n",
      "Yay, success!!\n",
      "Chart size: 58 edges\n",
      "2 edges ending in position 1:\n",
      "    [0-1: Det --> the . ]\n",
      "    [0-1: the -->  . ]\n",
      "3 edges ending in position 2:\n",
      "    [0-2: NP --> Det Noun . ]\n",
      "    [1-2: Noun --> lion . ]\n",
      "    [1-2: lion -->  . ]\n",
      "2 edges ending in position 13:\n",
      "    [12-13: Det --> the . ]\n",
      "    [12-13: the -->  . ]\n",
      "12 edges ending in position 14:\n",
      "    [0-14: S --> NP VP . ]\n",
      "    [2-14: VP --> VP PP . ]\n",
      "    [2-14: VP --> Verb NP . ]\n",
      "    [3-14: NP --> NP PP . ]\n",
      "    [5-14: PP --> Prep NP . ]\n",
      "    [6-14: NP --> NP PP . ]\n",
      "    [8-14: PP --> Prep NP . ]\n",
      "    [9-14: NP --> NP PP . ]\n",
      "    ...\n"
     ]
    }
   ],
   "source": [
    "test(earley1, grammar, 'S', example(3), positions=[1,2,-2,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 6 words: \"the lion sees a zebra under\"\n",
      "Meh, failure:(\n",
      "Chart size: 18 edges\n",
      "2 edges ending in position 1:\n",
      "    [0-1: Det --> the . ]\n",
      "    [0-1: the -->  . ]\n",
      "3 edges ending in position 2:\n",
      "    [0-2: NP --> Det Noun . ]\n",
      "    [1-2: Noun --> lion . ]\n",
      "    [1-2: lion -->  . ]\n",
      "4 edges ending in position 3:\n",
      "    [0-3: S --> NP VP . ]\n",
      "    [2-3: VP --> Verb . ]\n",
      "    [2-3: Verb --> sees . ]\n",
      "    [2-3: sees -->  . ]\n",
      "2 edges ending in position 4:\n",
      "    [3-4: Det --> a . ]\n",
      "    [3-4: a -->  . ]\n",
      "5 edges ending in position 5:\n",
      "    [0-5: S --> NP VP . ]\n",
      "    [2-5: VP --> Verb NP . ]\n",
      "    [3-5: NP --> Det Noun . ]\n",
      "    [4-5: Noun --> zebra . ]\n",
      "    [4-5: zebra -->  . ]\n",
      "2 edges ending in position 6:\n",
      "    [5-6: Prep --> under . ]\n",
      "    [5-6: under -->  . ]\n"
     ]
    }
   ],
   "source": [
    "test(earley1, grammar, 'S', example(1)[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation 2: Optimising the code\n",
    "\n",
    "All this is very good, but can we optimise the implementation, and if so how much? Let's test a very long sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 605 words: \"the lion sees ... in the park under a tree with a telescope\"\n",
      "Yay, success!!\n",
      "Chart size: 42216 edges\n",
      "406 edges ending in position 605:\n",
      "    [0-605: S --> NP VP . ]\n",
      "    [2-605: VP --> VP PP . ]\n",
      "    [2-605: VP --> Verb NP . ]\n",
      "    [3-605: NP --> NP PP . ]\n",
      "    [5-605: PP --> Prep NP . ]\n",
      "    [6-605: NP --> NP PP . ]\n",
      "    [8-605: PP --> Prep NP . ]\n",
      "    [9-605: NP --> NP PP . ]\n",
      "    ...\n",
      "CPU times: user 4.34 s, sys: 117 ms, total: 4.46 s\n",
      "Wall time: 4.59 s\n"
     ]
    }
   ],
   "source": [
    "%time test(earley1, grammar, 'S', example(200), positions=[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that `earley1` can parse a 600-word sentence in 4 seconds, building a chart of 42,000 passive edges. Can we beat that?\n",
    "\n",
    "There are some places where `earley1` does unnecessary work – **Predict** searches through the whole grammar just to find a couple of matching rules, and **Complete** searches through the whole edgeset *j*. The solution is to use suitable data structures:\n",
    "\n",
    "- **Predict**: Instead of searching through the whole grammar, we can use an optimised data structure. What we want to find is all rules that have the same left corner, so let's transform the grammar into a dictionary of the left corners. We already have a function for that, `leftcorner_dict`.\n",
    "\n",
    "- **Complete**: The same problem here – we have to loop through the whole edgeset *j*. Instead, we can convert the edgeset into a dictionary from a left corner to all edges with that left corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "metadata": {},
   "outputs": [],
   "source": [
    "def earley2(grammar, input):\n",
    "    # Transform the grammar into a more efficient data structure\n",
    "    leftcorners = leftcorner_dict(grammar)\n",
    "    \n",
    "    chart = [{None: set()}]\n",
    "    for k, sym in enumerate(input, 1):\n",
    "        # Divide the edgeset into a dictionary where the keys are the left corners of the edges\n",
    "        lc_edgesets = {}\n",
    "\n",
    "        # Scan\n",
    "        agenda = [ Edge(k-1, k, sym, (), 0) ]\n",
    "\n",
    "        while agenda:\n",
    "            edge = agenda.pop()\n",
    "            \n",
    "            # Get the edgeset for the given left corner\n",
    "            leftc = None if edge.passive() else edge.rhs[edge.dot]\n",
    "            edgeset = lc_edgesets.setdefault(leftc, set())\n",
    "            \n",
    "            # Add the edge to the edgeset, if it's not already there\n",
    "            if edge not in edgeset:\n",
    "                edgeset.add(edge)\n",
    "                \n",
    "                # If the edge is passive...\n",
    "                if edge.passive():\n",
    "                    # ...let's apply Predict:\n",
    "                    agenda.extend(\n",
    "                        Edge(edge.start, k, lhs, rhs, 1)\n",
    "                        for (lhs, rhs) in leftcorners.get(edge.lhs, [])\n",
    "                    )\n",
    "                    # ...and Complete:\n",
    "                    agenda.extend(\n",
    "                        Edge(e.start, k, e.lhs, e.rhs, e.dot+1) \n",
    "                        for e in chart[edge.start].get(edge.lhs, ())\n",
    "                    )\n",
    "        # Append the dictionary of edgesets to the chart\n",
    "        chart.append(lc_edgesets)\n",
    "    \n",
    "    # Return only the passive edges from the chart\n",
    "    return [ lc_edgesets[None] for lc_edgesets in chart ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the passive edges don't have a left corner, because they don't look for anything aymore. Instead we want to be able to filter out only the passive edges from a chart. For that reason we use a special symbol which cannot be a left corner, as key in the edgeset dictionary – and we choose the key `None` for these passive edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 605 words: \"the lion sees ... in the park under a tree with a telescope\"\n",
      "Yay, success!!\n",
      "Chart size: 42216 edges\n",
      "406 edges ending in position 605:\n",
      "    [0-605: S --> NP VP . ]\n",
      "    [2-605: VP --> VP PP . ]\n",
      "    [2-605: VP --> Verb NP . ]\n",
      "    [3-605: NP --> NP PP . ]\n",
      "    [5-605: PP --> Prep NP . ]\n",
      "    [6-605: NP --> NP PP . ]\n",
      "    [8-605: PP --> Prep NP . ]\n",
      "    [9-605: NP --> NP PP . ]\n",
      "    ...\n",
      "CPU times: user 2.26 s, sys: 10.2 ms, total: 2.27 s\n",
      "Wall time: 2.27 s\n"
     ]
    }
   ],
   "source": [
    "%time test(earley2, grammar, 'S', example(200), positions=[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Just by using better data structures, we made the implementation twice as fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation 3: Building parse trees\n",
    "\n",
    "Now we want to add some structure to the results, and we will use parse trees for that.\n",
    "\n",
    "Luckily the edges have an optional argument, `result`, which we can use. We will use this to store the parse trees that have been found by an edge. I.e., together with the edge [*i–k*: *A –> B1 ... Bm • Bn ... Bp*], we will store the trees that have been found for *B1 .... Bm*.\n",
    "\n",
    "The only changes to `earley2` is that **Scan**, **Predict** and **Complete** include parse trees in the newly created edges. Everything else stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "metadata": {},
   "outputs": [],
   "source": [
    "def earley3(grammar, input):\n",
    "    leftcorners = leftcorner_dict(grammar)    \n",
    "    chart = [{None: set()}]\n",
    "    for k, sym in enumerate(input, 1):\n",
    "        lc_edgesets = {}\n",
    "        \n",
    "        # Scan: since there are no children, the result will be the empty sequence ()\n",
    "        agenda = [ Edge(k-1, k, sym, (), 0, ()) ]\n",
    "\n",
    "        while agenda:\n",
    "            edge = agenda.pop()\n",
    "            leftc = None if edge.passive() else edge.rhs[edge.dot]\n",
    "            edgeset = lc_edgesets.setdefault(leftc, set())\n",
    "            if edge not in edgeset:\n",
    "                edgeset.add(edge)\n",
    "                \n",
    "                if edge.passive():\n",
    "                    # Build the tree for the recognised passive edge\n",
    "                    tree = Tree(edge.lhs, edge.result)\n",
    "\n",
    "                    # Predict: the new edge has recognised the passive edge,\n",
    "                    # so we add its tree to the results\n",
    "                    agenda.extend(\n",
    "                        Edge(edge.start, k, lhs, rhs, 1, (tree,))\n",
    "                        for (lhs, rhs) in leftcorners.get(edge.lhs, [])\n",
    "                    )\n",
    "                    # Complete: the new edge has recogninsed one more tree, \n",
    "                    # so we add it after the already recognised trees\n",
    "                    agenda.extend(\n",
    "                        Edge(e.start, k, e.lhs, e.rhs, e.dot+1, e.result+(tree,)) \n",
    "                        for e in chart[edge.start].get(edge.lhs, ())\n",
    "                    )\n",
    "        chart.append(lc_edgesets)\n",
    "    \n",
    "    return [ lc_edgesets[None] for lc_edgesets in chart ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what can we do with parse results? First, instead of just returning whether parsing succeeded, we can return all results that are found by succeeding passive edges.\n",
    "\n",
    "Note that we do not return a list of all results, but instead a generator. The reason is that we can stop the loop prematurely if we only need one result, and furthermore it also saves memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(chart, cat, start=0, end=-1):\n",
    "    for edge in chart[end]:\n",
    "        if edge.start == start and edge.lhs == cat and edge.passive():\n",
    "            yield edge.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the returned results are not the parse trees for the starting category, but the children trees. To get the final parse trees we have to combine the children with the starting category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trees3(chart, cat):\n",
    "    for children in results(chart, cat):\n",
    "        yield Tree(cat, children)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can implement a parser, taking a grammar, a starting category and the input sentence, yielding parse trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse3(grammar, cat, sentence):\n",
    "    chart = earley3(grammar, sentence)\n",
    "    return extract_trees3(chart, cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing \"the lion sees a zebra\":\n",
      "  1. S[NP[Det[the] Noun[lion]] VP[Verb[sees] NP[Det[a] Noun[zebra]]]]\n",
      "\n",
      "Parsing \"the lion sees a zebra under a tree\":\n",
      "  1. S[NP[Det[the] Noun[lion]] VP[VP[Verb[sees] NP[Det[a] Noun[zebra]]] PP[Prep[under] NP[Det[a] Noun[tree]]]]]\n",
      "  2. S[NP[Det[the] Noun[lion]] VP[Verb[sees] NP[NP[Det[a] Noun[zebra]] PP[Prep[under] NP[Det[a] Noun[tree]]]]]]\n",
      "\n",
      "Parsing \"the lion sees a zebra under a tree with a telescope\":\n",
      "  1. S[NP[Det[the] Noun[lion]] VP[VP[Verb[sees] NP[Det[a] Noun[zebra]]] PP[Prep[under] NP[NP[Det[a] Noun[tree]] PP[Prep[with] NP[Det[a] Noun[telescope]]]]]]]\n",
      "  2. S[NP[Det[the] Noun[lion]] VP[Verb[sees] NP[NP[NP[Det[a] Noun[zebra]] PP[Prep[under] NP[Det[a] Noun[tree]]]] PP[Prep[with] NP[Det[a] Noun[telescope]]]]]]\n",
      "  3. S[NP[Det[the] Noun[lion]] VP[VP[VP[Verb[sees] NP[Det[a] Noun[zebra]]] PP[Prep[under] NP[Det[a] Noun[tree]]]] PP[Prep[with] NP[Det[a] Noun[telescope]]]]]\n",
      "  4. S[NP[Det[the] Noun[lion]] VP[Verb[sees] NP[NP[Det[a] Noun[zebra]] PP[Prep[under] NP[NP[Det[a] Noun[tree]] PP[Prep[with] NP[Det[a] Noun[telescope]]]]]]]]\n",
      "  5. S[NP[Det[the] Noun[lion]] VP[VP[Verb[sees] NP[NP[Det[a] Noun[zebra]] PP[Prep[under] NP[Det[a] Noun[tree]]]]] PP[Prep[with] NP[Det[a] Noun[telescope]]]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(3):\n",
    "    print('Parsing \"%s\":' % (' '.join(example(n)),))\n",
    "    for i, tree in enumerate(parse3(grammar, 'S', example(n)), 1):\n",
    "        print('%3d. %s' % (i, tree))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... the number of parse trees seem to increas. Let's investigate their growth rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example    Length    Trees   T(n)/T(n-1)\n",
      "    1         8         2       2.0\n",
      "    2        11         5       2.5\n",
      "    3        14        14       2.8\n",
      "    4        17        42       3.0\n",
      "    5        20       132       3.1\n",
      "    6        23       429       3.2\n",
      "    7        26      1430       3.3\n",
      "    8        29      4862       3.4\n",
      "    9        32     16796       3.5\n",
      "   10        35     58786       3.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Example    Length    Trees   T(n)/T(n-1)\")\n",
    "oldtrees = 1\n",
    "for n in range(1, 11):\n",
    "    sent = example(n)\n",
    "    ntrees = len(list(parse3(grammar, 'S', sent)))\n",
    "    print(\"%5d%10d%10d%10.1f\" % (n, len(sent), ntrees, ntrees/oldtrees))\n",
    "    oldtrees = ntrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The growth rate seems to be converging on a constant, which means that the number of trees grows exponentially. In fact, the number of trees form the sequence of [Catalan numbers](https://en.wikipedia.org/wiki/Catalan_number), which is discussed further in the [NLTK book, chapter 8](https://www.nltk.org/book/ch08.html) (section 6.2 Pernicious Ambiguity).\n",
    "\n",
    "Now, what about the parsing efficiency? Let's test example 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 11 words: \"the lion sees a zebra under a tree with a telescope\"\n",
      "Yay, success!!\n",
      "Chart size: 51 edges\n",
      "18 edges ending in position 11:\n",
      "    [0-11: S --> NP VP . ]\n",
      "    [0-11: S --> NP VP . ]\n",
      "    [0-11: S --> NP VP . ]\n",
      "    [0-11: S --> NP VP . ]\n",
      "    [0-11: S --> NP VP . ]\n",
      "    [2-11: VP --> VP PP . ]\n",
      "    [2-11: VP --> VP PP . ]\n",
      "    [2-11: VP --> VP PP . ]\n",
      "    ...\n"
     ]
    }
   ],
   "source": [
    "test(earley3, grammar, 'S', example(2), positions=[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compare this with the previous parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 11 words: \"the lion sees a zebra under a tree with a telescope\"\n",
      "Yay, success!!\n",
      "Chart size: 42 edges\n",
      "10 edges ending in position 11:\n",
      "    [0-11: S --> NP VP . ]\n",
      "    [2-11: VP --> VP PP . ]\n",
      "    [2-11: VP --> Verb NP . ]\n",
      "    [3-11: NP --> NP PP . ]\n",
      "    [5-11: PP --> Prep NP . ]\n",
      "    [6-11: NP --> NP PP . ]\n",
      "    [8-11: PP --> Prep NP . ]\n",
      "    [9-11: NP --> Det Noun . ]\n",
      "    ...\n"
     ]
    }
   ],
   "source": [
    "test(earley2, grammar, 'S', example(2), positions=[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, our new parser creates more edges! In the final position, the new parser creates 18 edges, where the old one only created 10. And several edges seem to be equivalent... hmm, what's happening here?\n",
    "\n",
    "The problem is that since we now include the parse trees in the edges, every parse tree will have their own edge. You can see this above, where there are five *S–>NP VP* edges spanning the whole input, and in the table above we saw that the sentence has exactly five trees.\n",
    "\n",
    "Let's compare the chart sizes between `earley2` and `earley3`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   n      words     chart2     chart3\n",
      "   1        8         28         29\n",
      "   2       11         42         51\n",
      "   3       14         58         98\n",
      "   4       17         76        220\n",
      "   5       20         96        578\n",
      "   6       23        118       1704\n",
      "   7       26        142       5393\n",
      "   8       29        168      17805\n",
      "   9       32        196      60377\n",
      "  10       35        226     208587\n"
     ]
    }
   ],
   "source": [
    "print(\"   n      words     chart2     chart3\")\n",
    "for n in range(1, 11):\n",
    "    sent = example(n)\n",
    "    chart2 = earley2(grammar, sent)\n",
    "    chart3 = earley3(grammar, sent)\n",
    "    print(\"%4d %8d %10d %10d\" % (n, len(sent), chartsize(chart2), chartsize(chart3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look very good. The chart for `earley3` grows exponentially, compared to the `earley2` chart which seems to grow almost linearly. Let's test the parsing times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing earley2\n",
      "Parsing 35 words: \"the lion sees ... with a telescope in the park under a tree\"\n",
      "Yay, success!!\n",
      "Chart size: 226 edges\n",
      "CPU times: user 2.27 ms, sys: 129 µs, total: 2.4 ms\n",
      "Wall time: 2.42 ms\n",
      "\n",
      "Testing earley3\n",
      "Parsing 35 words: \"the lion sees ... with a telescope in the park under a tree\"\n",
      "Yay, success!!\n",
      "Chart size: 208587 edges\n",
      "CPU times: user 4.35 s, sys: 14.1 ms, total: 4.36 s\n",
      "Wall time: 4.36 s\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing earley2\")\n",
    "%time test(earley2, grammar, 'S', example(10), positions=[])\n",
    "print()\n",
    "print(\"Testing earley3\")\n",
    "%time test(earley3, grammar, 'S', example(10), positions=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently our attempt to include the parse trees in the edges didn't work out well. Can we do something else instead? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation 4: Building a forest\n",
    "\n",
    "Instead of collecting the parse trees, let's collect the recognised phrases.\n",
    "With that I mean to collect which child categories have been found, and where in the input. I.e., instead of storing parse trees, we store phrases, which are tuples of the form `(A,i,j)` (or `A{i-j}` as they are pretty-printed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phrase = namedtuple('Phrase', \"cat start end\")\n",
    "Phrase.__str__ = lambda phr: \"%s{%d-%d}\" % phr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference from before is that instead of storing full trees, we store phrases in **Scan**, **Predict** and **Complete**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "metadata": {},
   "outputs": [],
   "source": [
    "def earley4(grammar, input):\n",
    "    leftcorners = leftcorner_dict(grammar)    \n",
    "    chart = [{None: set()}]\n",
    "    for k, sym in enumerate(input, 1):\n",
    "        lc_edgesets = {}\n",
    "        \n",
    "        # Scan: since there are no children, the result will be the empty sequence ()\n",
    "        agenda = [ Edge(k-1, k, sym, (), 0, ()) ]\n",
    "\n",
    "        while agenda:\n",
    "            edge = agenda.pop()\n",
    "            leftc = None if edge.passive() else edge.rhs[edge.dot]\n",
    "            edgeset = lc_edgesets.setdefault(leftc, set())\n",
    "            if edge not in edgeset:\n",
    "                edgeset.add(edge)\n",
    "                \n",
    "                if edge.passive():\n",
    "                    # Build the phrase for the recognised passive edge\n",
    "                    phrase = Phrase(edge.lhs, edge.start, edge.end)\n",
    "\n",
    "                    # Predict: the new edge has recognised the passive edge,\n",
    "                    # so we add its phrase to the results\n",
    "                    agenda.extend(\n",
    "                        Edge(edge.start, k, lhs, rhs, 1, (phrase,))\n",
    "                        for (lhs, rhs) in leftcorners.get(edge.lhs, [])\n",
    "                    )\n",
    "                    # Complete: the new edge has recogninsed one more phrase, \n",
    "                    # so we add it after the already recognised phrases\n",
    "                    agenda.extend(\n",
    "                        Edge(e.start, k, e.lhs, e.rhs, e.dot+1, e.result+(phrase,)) \n",
    "                        for e in chart[edge.start].get(edge.lhs, ())\n",
    "                    )\n",
    "        chart.append(lc_edgesets)\n",
    "    \n",
    "    return [ lc_edgesets[None] for lc_edgesets in chart ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, did this change improve anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing earley3\n",
      "Parsing 35 words: \"the lion sees ... with a telescope in the park under a tree\"\n",
      "Yay, success!!\n",
      "Chart size: 208587 edges\n",
      "CPU times: user 3.3 s, sys: 9.79 ms, total: 3.31 s\n",
      "Wall time: 3.32 s\n",
      "\n",
      "Testing earley4\n",
      "Parsing 35 words: \"the lion sees ... with a telescope in the park under a tree\"\n",
      "Yay, success!!\n",
      "Chart size: 436 edges\n",
      "CPU times: user 3.68 ms, sys: 120 µs, total: 3.8 ms\n",
      "Wall time: 3.77 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing earley3\")\n",
    "%time test(earley3, grammar, 'S', example(10), positions=[])\n",
    "print()\n",
    "print(\"Testing earley4\")\n",
    "%time test(earley4, grammar, 'S', example(10), positions=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems so! But now we don't have any parse trees, just phrases – can we build trees from these phrases?\n",
    "\n",
    "Yes we can, by first converting the final chart to a \"parse forest\". A forest is really a kind of context-free grammar, where the symbols are phrases. So first we convert the chart to a forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 990,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_forest4(chart):\n",
    "    return [ Rule(Phrase(edge.lhs, edge.start, edge.end), edge.result)\n",
    "             for edgeset in chart\n",
    "             for edge in edgeset\n",
    "             if edge.passive()\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest after parsing \"the lion sees a zebra\":\n",
      "   the{0-1} --> \n",
      "   Det{0-1} --> the{0-1}\n",
      "   NP{0-2} --> Det{0-1} Noun{1-2}\n",
      "   lion{1-2} --> \n",
      "   Noun{1-2} --> lion{1-2}\n",
      "   VP{2-3} --> Verb{2-3}\n",
      "   S{0-3} --> NP{0-2} VP{2-3}\n",
      "   Verb{2-3} --> sees{2-3}\n",
      "   sees{2-3} --> \n",
      "   a{3-4} --> \n",
      "   Det{3-4} --> a{3-4}\n",
      "   VP{2-5} --> Verb{2-3} NP{3-5}\n",
      "   Noun{4-5} --> zebra{4-5}\n",
      "   NP{3-5} --> Det{3-4} Noun{4-5}\n",
      "   S{0-5} --> NP{0-2} VP{2-5}\n",
      "   zebra{4-5} --> \n"
     ]
    }
   ],
   "source": [
    "print('Forest after parsing \"%s\":' % ' '.join(example(0)))\n",
    "forest = extract_forest4(earley4(grammar, example(0)))\n",
    "for rule in forest:\n",
    "    print(\"  \", rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the trees from a forest\n",
    "\n",
    "From this parse forest we can extract all parse trees, by doing a nondeterministic recursive-descent enumeration. We define two mutually recursive functions, `yield_tree` and `yield_children`:\n",
    "\n",
    "- `yield_tree` takes a phrase *A*{*i–k*} as input and calculates all trees in the forest, with *A* as root category and spanning the positions *i–k*. It does this by calling `yield_children` to get all possible combinations of children trees.\n",
    "- `yield_children` takes a sequence of phrases as input. It goes through each phrase in turn, calculates a tree for that phrase, and then children trees for the remaining phrases, and finally combines them.\n",
    "\n",
    "Since both functions are nondeterminstic, we use the `yield` keyword instead of `return`, which makes them yield results several times. \n",
    "\n",
    "One optimisation that we do is to convert the forest into a dictionary where we can look up the left-hand side phrases of the forest. This is done by the function `topdown_dict` that we defined in the beginning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 999,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trees4(forest, cat, start=0, end=None):\n",
    "    topdowns = topdown_dict(forest)\n",
    "    if end is None:\n",
    "        end = max(lhs.end for lhs in topdowns)\n",
    "\n",
    "    def yield_tree(lhs):\n",
    "        # For every rule in the forest that has 'lhs' as left-hand side...\n",
    "        for rule in topdowns.get(lhs, ()):\n",
    "            # ...and for every combination of children trees...\n",
    "            for children in yield_children(rule.rhs, 0):\n",
    "                # ...build a tree for the given input:\n",
    "                yield Tree(lhs.cat, children)\n",
    "\n",
    "    def yield_children(rhs, i):\n",
    "        if i == len(rhs):\n",
    "            # If there are no phrases in 'rhs' to process, we yield the empty sequence of trees:\n",
    "            yield ()\n",
    "        else:\n",
    "            # Otherwise, for every possible tree for the phrase at position 'i'...\n",
    "            for tree in yield_tree(rhs[i]):\n",
    "                # ...and for every combination of trees for the rest of the phrases...\n",
    "                for trees in yield_children(rhs, i+1):\n",
    "                    # ...add the tree in front of the trees:\n",
    "                    yield (tree,) + trees\n",
    "    \n",
    "    return yield_tree(Phrase(cat, start, end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a new parsing function, that first parses to a chart, then converts the chart to a forest, and after that extracts the trees, one at the time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse4(grammar, cat, sentence):\n",
    "    chart = earley4(grammar, sentence)\n",
    "    forest = extract_forest4(chart)\n",
    "    return extract_trees4(forest, cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing \"the lion sees a zebra\":\n",
      "  1. S[NP[Det[the] Noun[lion]] VP[Verb[sees] NP[Det[a] Noun[zebra]]]]\n",
      "\n",
      "Parsing \"the lion sees a zebra under a tree\":\n",
      "  1. S[NP[Det[the] Noun[lion]] VP[Verb[sees] NP[NP[Det[a] Noun[zebra]] PP[Prep[under] NP[Det[a] Noun[tree]]]]]]\n",
      "  2. S[NP[Det[the] Noun[lion]] VP[VP[Verb[sees] NP[Det[a] Noun[zebra]]] PP[Prep[under] NP[Det[a] Noun[tree]]]]]\n",
      "\n",
      "Parsing \"the lion sees a zebra under a tree with a telescope\":\n",
      "  1. S[NP[Det[the] Noun[lion]] VP[VP[Verb[sees] NP[Det[a] Noun[zebra]]] PP[Prep[under] NP[NP[Det[a] Noun[tree]] PP[Prep[with] NP[Det[a] Noun[telescope]]]]]]]\n",
      "  2. S[NP[Det[the] Noun[lion]] VP[Verb[sees] NP[NP[NP[Det[a] Noun[zebra]] PP[Prep[under] NP[Det[a] Noun[tree]]]] PP[Prep[with] NP[Det[a] Noun[telescope]]]]]]\n",
      "  3. S[NP[Det[the] Noun[lion]] VP[Verb[sees] NP[NP[Det[a] Noun[zebra]] PP[Prep[under] NP[NP[Det[a] Noun[tree]] PP[Prep[with] NP[Det[a] Noun[telescope]]]]]]]]\n",
      "  4. S[NP[Det[the] Noun[lion]] VP[VP[Verb[sees] NP[NP[Det[a] Noun[zebra]] PP[Prep[under] NP[Det[a] Noun[tree]]]]] PP[Prep[with] NP[Det[a] Noun[telescope]]]]]\n",
      "  5. S[NP[Det[the] Noun[lion]] VP[VP[VP[Verb[sees] NP[Det[a] Noun[zebra]]] PP[Prep[under] NP[Det[a] Noun[tree]]]] PP[Prep[with] NP[Det[a] Noun[telescope]]]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(3):\n",
    "    print('Parsing \"%s\":' % (' '.join(example(n)),))\n",
    "    for i, tree in enumerate(parse4(grammar, 'S', example(n)), 1):\n",
    "        print('%3d. %s' % (i, tree))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that worked. How about efficiency, compared to our previous parser?\n",
    "Let's see how long time it takes to first to parse a sentence, then extract the first tree, then the second tree, then another 1000 trees. It's important that we don't build the whole list of trees, so we use the `islice` function to extract the 1000 trees without exhausting the trees iterator.\n",
    "\n",
    "First, we try our previous parser, `parse3`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing\n",
      "CPU times: user 4.4 s, sys: 15.5 ms, total: 4.41 s\n",
      "Wall time: 4.42 s\n",
      "\n",
      "Tree 1: S[NP[Det[the] Noun[lion]] VP[VP[VP[VP[Verb[sees] NP[NP[Det[a] Noun[zebra]] PP[Pr...\n",
      "CPU times: user 185 µs, sys: 3 µs, total: 188 µs\n",
      "Wall time: 193 µs\n",
      "\n",
      "Tree 2: S[NP[Det[the] Noun[lion]] VP[VP[Verb[sees] NP[NP[Det[a] Noun[zebra]] PP[Prep[und...\n",
      "CPU times: user 238 µs, sys: 56 µs, total: 294 µs\n",
      "Wall time: 301 µs\n",
      "\n",
      "Next 1000 trees: 1000\n",
      "CPU times: user 2.34 ms, sys: 64 µs, total: 2.4 ms\n",
      "Wall time: 2.42 ms\n"
     ]
    }
   ],
   "source": [
    "%time print(\"Parsing\"); trees = parse3(grammar, 'S', example(10))\n",
    "%time print(\"\\nTree 1: %.80s...\" % (next(trees),))\n",
    "%time print(\"\\nTree 2: %.80s...\" % (next(trees),))\n",
    "%time print(\"\\nNext 1000 trees: %d\" % (len(list(islice(trees, 1000)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parsing takes a lot of time, but then extracting the trees is instantaneous. \n",
    "\n",
    "How about our new parser, `parse4`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing\n",
      "CPU times: user 88 ms, sys: 1.4 ms, total: 89.4 ms\n",
      "Wall time: 89 ms\n",
      "\n",
      "Tree 1: S[NP[Det[the] Noun[lion]] VP[VP[VP[VP[Verb[sees] NP[Det[a] Noun[zebra]]] PP[Prep...\n",
      "CPU times: user 702 µs, sys: 792 µs, total: 1.49 ms\n",
      "Wall time: 2.1 ms\n",
      "\n",
      "Tree 2: S[NP[Det[the] Noun[lion]] VP[VP[VP[VP[Verb[sees] NP[Det[a] Noun[zebra]]] PP[Prep...\n",
      "CPU times: user 201 µs, sys: 5 µs, total: 206 µs\n",
      "Wall time: 208 µs\n",
      "\n",
      "Next 1000 trees: 1000\n",
      "CPU times: user 94.4 ms, sys: 2.54 ms, total: 96.9 ms\n",
      "Wall time: 95 ms\n"
     ]
    }
   ],
   "source": [
    "%time print(\"Parsing\"); trees = parse4(grammar, 'S', example(10))\n",
    "%time print(\"\\nTree 1: %.80s...\" % (next(trees),))\n",
    "%time print(\"\\nTree 2: %.80s...\" % (next(trees),))\n",
    "%time print(\"\\nNext 1000 trees: %d\" % (len(list(islice(trees, 1000)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that was good! Parsing is suddenly super-fast, but as you can see, extracting each tree takes slightly longer time than before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Even more optimisations\n",
    "\n",
    "Are we done now? Not quite... let's compare our latest parser `earley4` with our optimised `earley2` (which did not store any parse results), on a 300-word sentence. We don't extract any trees, just build the chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing earley2\n",
      "Parsing 305 words: \"the lion sees ... with a telescope in the park under a tree\"\n",
      "Yay, success!!\n",
      "Chart size: 11116 edges\n",
      "CPU times: user 320 ms, sys: 3.76 ms, total: 324 ms\n",
      "Wall time: 322 ms\n",
      "\n",
      "Testing earley4\n",
      "Parsing 305 words: \"the lion sees ... with a telescope in the park under a tree\"\n",
      "Yay, success!!\n",
      "Chart size: 182716 edges\n",
      "CPU times: user 2.36 s, sys: 6.71 ms, total: 2.37 s\n",
      "Wall time: 2.37 s\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing earley2\")\n",
    "%time test(earley2, grammar, 'S', example(100), positions=[])\n",
    "print()\n",
    "print(\"Testing earley4\")\n",
    "%time test(earley4, grammar, 'S', example(100), positions=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... it seems we're still not there. Although the new parser `earley4` is a huge improvement compared to `earley3`, it is still a magnitude slower than `earley2`. \n",
    "\n",
    "Why is that? It's because there can be several ways to combine children phrases into one parent phrase. E.g., the phrase NP{3–305} can be derived from the phrases NP{3–5} PP{5–305}, or NP{3–8} PP{8–305}, or ..., or NP{3–302} PP{302–305}. All these combinations result in one edge each, wheras `earley2` would only create one NP edge spanning 3–305.\n",
    "\n",
    "So, can we do something about this? Well, we can try to convert the chart returned by `earley2` into a parse forest. The edges now don't contain any children phrases, so instead we have to calculate them on the fly when building the forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_forest2b(chart):\n",
    "    # First we build a dictionary from the chart,\n",
    "    # which maps (lhs,start) into all possible end positions.\n",
    "    topdowns = {}\n",
    "    for edgeset in chart:\n",
    "        for edge in edgeset:\n",
    "            if edge.passive():\n",
    "                topdowns.setdefault((edge.lhs, edge.start), set()) \\\n",
    "                    .add(Phrase(edge.lhs, edge.start, edge.end))\n",
    "\n",
    "    # This function annotates the given right-hand side as a sequence of phrases.\n",
    "    # This is nondeterministic, since one right-hand side can be recognised using different spans.\n",
    "    def collect_phrases(rhs, dot, start, end):\n",
    "        if not rhs:\n",
    "            yield ()\n",
    "        elif start == end and dot == len(rhs):\n",
    "            yield ()\n",
    "        elif start < end and dot < len(rhs):\n",
    "            for phr in topdowns.get((rhs[dot], start), ()):\n",
    "                for phrases in collect_phrases(rhs, dot+1, phr.end, end):\n",
    "                    yield (phr,) + phrases\n",
    "\n",
    "    # Now we can transform each edge in the chart into (possibly several) forest rules.\n",
    "    return [ Rule(Phrase(edge.lhs, edge.start, edge.end), phrases)\n",
    "             for edgeset in chart\n",
    "             for edge in edgeset\n",
    "             if edge.passive()\n",
    "             for phrases in collect_phrases(edge.rhs, 0, edge.start, edge.end)\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1032,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Det{0-1} --> the{0-1}\n",
      "the{0-1} --> \n",
      "Noun{1-2} --> lion{1-2}\n",
      "lion{1-2} --> \n",
      "NP{0-2} --> Det{0-1} Noun{1-2}\n",
      "Verb{2-3} --> sees{2-3}\n",
      "VP{2-3} --> Verb{2-3}\n",
      "S{0-3} --> NP{0-2} VP{2-3}\n",
      "sees{2-3} --> \n",
      "Det{3-4} --> a{3-4}\n",
      "a{3-4} --> \n",
      "VP{2-5} --> Verb{2-3} NP{3-5}\n",
      "NP{3-5} --> Det{3-4} Noun{4-5}\n",
      "S{0-5} --> NP{0-2} VP{2-5}\n",
      "zebra{4-5} --> \n",
      "Noun{4-5} --> zebra{4-5}\n"
     ]
    }
   ],
   "source": [
    "forest = extract_forest2b(earley2(grammar, example(0)))\n",
    "for rule in forest:\n",
    "    print(rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly the same forest as when we tested `extract_forest4` previously. \n",
    "\n",
    "Now we can define a parser which uses this forest extraction and then extracts the trees from the forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1033,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse2b(grammar, cat, sentence):\n",
    "    chart = earley2(grammar, sentence)\n",
    "    forest = extract_forest2b(chart)\n",
    "    return extract_trees4(forest, cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And behold, it works too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1034,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S[NP[Det[the] Noun[lion]] VP[Verb[sees] NP[NP[Det[a] Noun[zebra]] PP[Prep[under] NP[Det[a] Noun[tree]]]]]]\n",
      "S[NP[Det[the] Noun[lion]] VP[VP[Verb[sees] NP[Det[a] Noun[zebra]]] PP[Prep[under] NP[Det[a] Noun[tree]]]]]\n"
     ]
    }
   ],
   "source": [
    "for tree in parse2b(grammar, 'S', example(1)):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, have we gained anything, when it comes to efficiency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing\n",
      "CPU times: user 5.66 s, sys: 17.7 ms, total: 5.67 s\n",
      "Wall time: 5.69 s\n",
      "\n",
      "Tree 1: S[NP[Det[the] Noun[lion]] VP[Verb[sees] NP[NP[NP[NP[Det[a] Noun[zebra]] PP[Prep[...\n",
      "CPU times: user 3.53 ms, sys: 60 µs, total: 3.59 ms\n",
      "Wall time: 3.61 ms\n",
      "\n",
      "Tree 2: S[NP[Det[the] Noun[lion]] VP[Verb[sees] NP[NP[NP[NP[Det[a] Noun[zebra]] PP[Prep[...\n",
      "CPU times: user 927 µs, sys: 34 µs, total: 961 µs\n",
      "Wall time: 971 µs\n",
      "\n",
      "Next 1000 trees: 1000\n",
      "CPU times: user 145 ms, sys: 1.43 ms, total: 146 ms\n",
      "Wall time: 146 ms\n"
     ]
    }
   ],
   "source": [
    "%time print(\"Parsing\"); trees = parse2b(grammar, 'S', example(100))\n",
    "%time print(\"\\nTree 1: %.80s...\" % (next(trees),))\n",
    "%time print(\"\\nTree 2: %.80s...\" % (next(trees),))\n",
    "%time print(\"\\nNext 1000 trees: %d\" % (len(list(islice(trees, 1000)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh... this was even worse than before. `parse2b` took 4 seconds, and this was 50% slower.\n",
    "\n",
    "It's extracting the forest that takes time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling earley2\n",
      "CPU times: user 318 ms, sys: 3.18 ms, total: 321 ms\n",
      "Wall time: 320 ms\n",
      "\n",
      "Calling extract_forest2\n",
      "CPU times: user 5.64 s, sys: 18 ms, total: 5.65 s\n",
      "Wall time: 5.67 s\n"
     ]
    }
   ],
   "source": [
    "%time print(\"Calling earley2\"); chart = earley2(grammar, example(100))\n",
    "print()\n",
    "%time print(\"Calling extract_forest2\"); forest = extract_forest2b(chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the trees directly from the chart\n",
    "\n",
    "So, what can we do about this? We can combine `extract_forest2b` with `extract_trees4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trees2c(chart, cat, start=0, end=None):\n",
    "    if end is None:\n",
    "        end = len(chart) - 1\n",
    "\n",
    "    # We build a dictionary which maps (lhs,start) into all possible end positions.\n",
    "    topdowns = {}\n",
    "    for edgeset in chart:\n",
    "        for edge in edgeset:\n",
    "            if edge.passive():\n",
    "                topdowns.setdefault((edge.lhs, edge.start), []).append(edge)\n",
    "\n",
    "    # Build trees for category 'lhs', that start in position 'start'.\n",
    "    # We also have a predicate 'test_end' which returns False if we have passed the end of the phrase.\n",
    "    def yield_tree(lhs, start, test_end):\n",
    "        # For every rule in the forest that has 'lhs' as left-hand side...\n",
    "        for edge in topdowns.get((lhs, start), ()):\n",
    "            # ...such that it does not go pass the end of the phrase...\n",
    "            if test_end(edge.end):\n",
    "                # ...and for every combination of children trees...\n",
    "                for children in yield_children(edge.rhs, 0, start, edge.end):\n",
    "                    # ...build a tree for the given input:\n",
    "                    yield Tree(lhs, children), edge.end\n",
    "\n",
    "    def yield_children(rhs, dot, start, end):\n",
    "        if not rhs:\n",
    "            # If this is an edge created by Scan, yield the empty sequence of trees:\n",
    "            yield ()\n",
    "        elif start == end and dot == len(rhs):\n",
    "            # If there are no phrases in 'rhs' to process, yield the empty sequence of trees:\n",
    "            yield ()\n",
    "        elif start < end and dot < len(rhs):\n",
    "            # If there is only one symbol left in 'rhs', the next phrase have to end in 'end',\n",
    "            # otherwise the next phrase have to end somewhere before 'end':\n",
    "            test_end = (lambda e:e==end) if dot == len(rhs)-1 else (lambda e:e<end)\n",
    "            # For every possible tree for the phrase at position 'dot'...\n",
    "            for tree, mid in yield_tree(rhs[dot], start, test_end):\n",
    "                # ...and for every combination of trees for the rest of the phrases...\n",
    "                for trees in yield_children(rhs, dot+1, mid, end):\n",
    "                    # ...add the tree in front of the trees:\n",
    "                    yield (tree,) + trees\n",
    "\n",
    "    for tree, _ in yield_tree(cat, start, lambda e:e==end):\n",
    "        yield tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1038,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse2c(grammar, cat, sentence):\n",
    "    chart = earley2(grammar, sentence)\n",
    "    return extract_trees2c(chart, cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing\n",
      "CPU times: user 326 ms, sys: 4.45 ms, total: 330 ms\n",
      "Wall time: 332 ms\n",
      "\n",
      "Tree 1: S[NP[Det[the] Noun[lion]] VP[Verb[sees] NP[NP[Det[a] Noun[zebra]] PP[Prep[under]...\n",
      "CPU times: user 15.5 ms, sys: 298 µs, total: 15.8 ms\n",
      "Wall time: 15.8 ms\n",
      "\n",
      "Tree 2: S[NP[Det[the] Noun[lion]] VP[Verb[sees] NP[NP[Det[a] Noun[zebra]] PP[Prep[under]...\n",
      "CPU times: user 2.14 ms, sys: 116 µs, total: 2.26 ms\n",
      "Wall time: 2.28 ms\n",
      "\n",
      "Next 1000 trees: 1000\n",
      "CPU times: user 2.37 s, sys: 7.4 ms, total: 2.37 s\n",
      "Wall time: 2.37 s\n"
     ]
    }
   ],
   "source": [
    "%time print(\"Parsing\"); trees = parse2c(grammar, 'S', example(100))\n",
    "%time print(\"\\nTree 1: %.80s...\" % (next(trees),))\n",
    "%time print(\"\\nTree 2: %.80s...\" % (next(trees),))\n",
    "%time print(\"\\nNext 1000 trees: %d\" % (len(list(islice(trees, 1000)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're there! Parsing is very fast, and building every tree is also very fast. To build very many trees still takes time, but we probably don't want 1000 trees to choose from. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "### Using a function for categorising the words\n",
    "\n",
    "### Allowing empty grammar rules\n",
    "\n",
    "### Adding probabilities to the grammar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
